Como última iteración, se experimentó con una red menos profunda de dos capas. La red convolucional cnn\_two se estableció como la arquitectura base, sus hiperparámetros y estructura se describen en la tabla \ref{tbl:11}. A partir de esta arquitectura, se añadieron técnicas de regularización dropout en los modelos propuestos, cuyos resultados se detallan en la tabla algo \ref{tbl:12}.

\begin{itemize}

\item Al modelo cnn\_dp\_two se le aplicó un dropout del 30\% después de cada capa convolucional y un dropout del 50\% después de la capa densa intermedia.

\item Al modelo cnn\_dp\_two1 se le aplicó un dropout del 10\% después de cada capa convolucional y un dropout del 20\% después de la capa densa intermedia.
\end{itemize}

\begin{table}[!ht]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Tipo de capa} & \textbf{Hiperparámetro } & \textbf{Valor} \\ \hline
		1ra capa convolucional 1D & um\_filtros, tam\_filtros, stride & 64, 3, 1 \\ \hline
		1ra capa maxpooling & tam\_pool, pool\_stride & 2, 1 \\ \hline
		2da capa convolucional 1D & num\_filtros, tam\_filtros, stride & 128, 5, 1 \\ \hline
		1ra capa densa intermedia & num\_neuronas & 128 \\ \hline
		2da capa densa de salida & num\_neuronas & 3 \\ \hline
	\end{tabular}
	\caption{Detalle red convolucional cnn\_two}
	\label{tbl:11}
\end{table}


\begin{table}[!ht]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{Nombre del modelo} & \textbf{Precisión} & \textbf{Perdida} & \textbf{Val\_Precisión} & \textbf{Val\_Perdida} & \textbf{Epoca} \\ \hline
		cnn\_two & 0.7686 & 0.56 & 0.7787 & 0.5356 & 3 \\ \cline{2-6}
		~ & 0.9765 & 0.06 & 0.7004 & 2.2804 & 50 \\ \hline
		cnn\_dp\_two & 0.7850 & 0.53 & 0.7718 & 0.5523 & 8 \\ \cline{2-6}
		~ & 0.8718 & 0.32 & 0.7441 & 0.7408 & 50 \\ \hline
		cnn\_dp\_two1 & 0.7920 & 0.51 & 0.7808 & 0.5424 & 6 \\ \cline{2-6}
		~ & 0.1942 & 0.92 & 0.7303 & 1.0931 & 50 \\ \hline
	\end{tabular}
	\caption{Detalle resultados de técnicas de regularización dropout}
	\label{tbl:12}
\end{table}
